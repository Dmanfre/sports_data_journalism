```{r}
#loading libraries

library(tidyverse)
library(Hmisc)
```

```{r}

#loading data
logs <- read_csv("https://dwillis.github.io/sports-data-files/cbblogs1523.csv")


```

```{r}
#build a model to see if we can determine score differential between two teams

logs <- logs %>% 
  mutate( Differential = TeamScore - OpponentScore, 
          NetRebounds = TeamTotalRebounds - OpponentTotalRebounds,
          TurnoverMargin = TeamTurnovers - OpponentTurnovers)



```

```{r}

rebounds <- lm(Differential ~ NetRebounds, data=logs)
summary(rebounds)

# The p value is very low, and the r squared value is cloesr to 0, There is a relationship but the correlation is not strong because the r-squared value. Essentially what this says is that net rebounds does not determine the predictability the point differential of a game. 

```

```{r}
#This means we ned to add more to our model

rebounds_turnovers <- lm(Differential ~ NetRebounds + TurnoverMargin, data=logs)
summary(rebounds_turnovers)

# The R-Squared is much closer to 1 which means that there is a much stronger correlation.
#The P-value is still very small
# Knowing net rebound and trunover margin gets is over 50% in predicibility power.

# Residual standard error: the smaller the number is the better the model is. This produces 10.29 which is low and means we are doing the right thing. 

# Can this be improved by adding something else?

rebounds_turnovers <- lm(Differential ~ NetRebounds + TurnoverMargin, data=logs)
summary(rebounds_turnovers)

```

```{r}
#Hmis library shows you what to put into the model and what you should not put into the model. 

simplelogs <- logs %>% 
  select_if(is.numeric) %>% 
  select(-Game) %>% 
  select(Differential, NetRebounds, TurnoverMargin, TeamFGPCT, TeamTotalRebounds, OpponentFGPCT, OpponentTotalRebounds)


summary(simplelogs)

# Ask the model to build a model showing how each variable is related to each other. 

cormatrix <- rcorr(as.matrix(simplelogs))

cormatrix$r
#This matrix compares each of the variables against itself.
# The way we should be looking at this is the following: theyre between -1 and 1. 1= perfect positive correlation and -1 is a perfect negative correlation
#We want to take a positive correlation but is not related... answer is TeamFGPCT when correlating to Differential.
# You would also include OpponentFGPCT here becasue that is also not related to differential
```

```{r}

model2 <- lm(Differential ~ NetRebounds + TurnoverMargin + TeamFGPCT + OpponentFGPCT, data=logs)
summary(model2)

# This code block shows us a certain degree of confidence in an outcome of a game.
# The p value is low, R Squared is 0.89 

```



```{r}
# Saying these teams: on average how did they do? on average how did they do in these categories?

logs |> 
  filter(Team == "Michigan" & Season == '2020-2021' | Team == "Wisconsin" & Season == '2019-2020' | Team == "Michigan State" & Season == '2018-2019' | Team == "Michigan State" & Season == '2017-2018' | Team == 'Illinois' & Season == '2021-2022' | Team == 'Purdue' & Season == '2022-2023') |> 
  summarise(
    meanNetRebounds = mean(NetRebounds),
    meanTurnoverMargin = mean(TurnoverMargin),
    meanTeamFGPCT = mean(TeamFGPCT),
    meanOpponentFGPCT = mean(OpponentFGPCT)
  )

#Now we plug them into the model, is the following 

# (netrebounds estimate * meanNetRebounds) + (turnover margin estimate * meanTurnoverMargin) + (TeamFGPCT estimate * meanTeamFGPCT) + (OpponentFGPCT estimate * meanOpponentFGPCT) + Intercept
(0.654800*6.05) + (-1.310579*0.6333333) + (90.805990*0.4543167) + (-91.351310*0.4107167) + 0.287665

#this uses the y=mx+b model
#They should outscore their opponents by 7.154341 points per game over the course of a season.



#How does this compare to Maryland in the previous seaosn

logs |> 
  filter(
    Team == "Maryland" & Season == '2022-2023'
    ) |> 
  summarise(
    meanNetRebounds = mean(NetRebounds),
    meanTurnoverMargin = mean(TurnoverMargin),
    meanTeamFGPCT = mean(TeamFGPCT),
    meanOpponentFGPCT = mean(OpponentFGPCT)
  )
#This says Maryland should slightly outrebound opponents



#using this equation to predict scoring. 
(0.654800*1.685714) + (-1.310579*0.9142857) + (90.805990*0.4517714) + (-91.351310*0.428) + 0.287665
#This says we should outscore out oppoenents by 2.118415

logs |> 
     filter(
         Team == "Maryland" & Season == '2022-2023'
     ) |> summarise(avg_score = mean(TeamScore), avg_opp = mean(OpponentScore))

# Maryland actually outscored its opponents by 6 points. 


```


Residual lessons from Sept. 28


```{r}

residualmodel <- logs |> mutate(differential = TeamScore - OpponentScore, FGPctMargin = TeamFGPCT - OpponentFGPCT)

fit <- lm(differential ~ FGPctMargin, data = residualmodel)
summary(fit)

```
This is a pretty predictive model becasue its saying you have a better chance of winning if you shoot better.
Looking at the residuals: There is a range of them from smallest to largest value

Residuals are the difference between the prediction and what actually happened. 

```{r}
residualmodel <- residualmodel %>% na.omit()

residualmodel <- residualmodel |> mutate(predicted = predict(fit), residuals = residuals(fit))

#This filters out all the ones where the FGP is
residualmodel <- residualmodel |> filter(!is.na(FGPctMargin))

```


Working with Z - Scores from Sept. 28

z scores show a team or player is better than their opponents or peers. 
Be careful using one sesaon... make sure to try and use multiple seasons. 

Create the average first and then do the Z score. ORDER MATTERS. 

```{r}

gamelogs <- read_csv("https://dwillis.github.io/sports-data-files/wbblogs23.csv")

```

```{r}
#Chose these becasue that is what Derek defines as "quality"
teamquality <- gamelogs |> 
  select(Conference, Team, TeamFGPCT, TeamTotalRebounds, OpponentFGPCT, OpponentTotalRebounds)

```


```{r}
#Calculating averages

teamtotals <- teamquality |> 
  group_by(Conference, Team) |> 
  summarise(
    FGAvg = mean(TeamFGPCT), 
    ReboundAvg = mean(TeamTotalRebounds), 
    OppFGAvg = mean(OpponentFGPCT),
    OppRebAvg = mean(OpponentTotalRebounds)
    ) 

```

z scores calculate the number of standard deviations from the mean. 
Negatives numbers mean they are X standard deviations below the average.
```{r}
teamzscore <- teamtotals |> 
  mutate(
    FGzscore = as.numeric(scale(FGAvg, center = TRUE, scale = TRUE)),
    RebZscore = as.numeric(scale(ReboundAvg, center = TRUE, scale = TRUE)),
    OppZscore = as.numeric(scale(OppFGAvg, center = TRUE, scale = TRUE)) * -1,
    OppRebZScore = as.numeric(scale(OppRebAvg, center = TRUE, scale = TRUE)) * -1,
    TotalZscore = FGzscore + RebZscore + OppZscore + OppRebZScore
  )  

# showing the best TeamScore team
teamzscore |> arrange(desc(TotalZscore))
```


Picking metrics matters here becuase Maryland is low but its because they didnt rebound.... the RebPerc hurt them in this metric. 

MSU being at 0.0 menas they are at average among the Big Ten in those metrics. 
```{r}
#Filtering just the Big Ten

teamzscore |> 
  filter(Conference == "MAAC WBB") |> 
  arrange(desc(TotalZscore)) |>
  select(Team, TotalZscore)

```



```{r}
#Limiting the conference to the P5 and The Big East
powerfive_plus_one <- c("SEC WBB", "Big Ten WBB", "Pac-12 WBB", "Big 12 WBB", "ACC WBB", "Big East WBB")
teamzscore |> 
  filter(Conference %in% powerfive_plus_one) |> 
  arrange(desc(TotalZscore)) |>
  select(Team, TotalZscore)



#just becasue
powerfive_plus_maac_beast <- c("SEC WBB", "Big Ten WBB", "Pac-12 WBB", "Big 12 WBB", "ACC WBB", "Big East WBB", "MAAC WBB")
teamzscore |> 
  filter(Conference %in% powerfive_plus_maac_beast) |> 
  arrange(desc(TotalZscore)) |>
  select(Team, TotalZscore)

```


